# -*- coding: utf-8 -*-
"""pca.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oF_GRtYW3AlXGwqSsQFMvW-AACh2HcRn

#Principal Component Analysis (PCA)

In this notebook we will following a [tutorial](https://towardsdatascience.com/a-complete-guide-to-principal-component-analysis-pca-in-machine-learning-664f34fc3e5a) to show how to apply PCA to reduce dimensionality in datasets.

Steps involved in PCA

> * Standardize the data
> * Compute covariance matrix
> * Obtain the Eigenvectors and Eigenvalues from the covariance matrix
> * Sort eigenvalues in descending order and choose the top k Eigenvectors that correspond to the k largest eigenvalues
> * nstruct the projection matrix W from the selected k Eigenvectors
> * Transform the original data set X via W to obtain the new k-dimensional feature subspace Y

## Importing Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt 
from sklearn.decomposition import PCA 
from sklearn.preprocessing import StandardScaler 
# %matplotlib inline

"""## Iris Data

Reading Iris dataset from UCI
"""

print("PCA to Iris Data\n")

df = pd.read_csv(
    filepath_or_buffer='https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data',
    header=None,
    sep=',')

"""## Processing dataset

Setting columns name
"""

df.columns=['sepal_len', 'sepal_wid', 'petal_len', 'petal_wid', 'class']

"""Drop empty rows"""

df.dropna(how="all", inplace=True)

"""Let's see last five rows"""

print("Let's see last five rows\n\n{0}".format(df.tail()))

"""Separate features and target columns"""

X = df.iloc[:,0:4].values # Features
y = df.iloc[:,4].values # Target

"""Features values"""

print("\n\nFeatures values\n\n{}".format(X))

"""Target values"""

print("\n\nTarget values\n\n{}".format(y))

"""## Standardization

Sometimes, the scales used for the measurement in each feature can be different. In this cases we can standarize ***(mean=0, variance=1)*** the dataset because PCA is sensitive to variances
"""

X_std = StandardScaler().fit_transform(X)

print("\n\nX Standarized\n\n{}".format(X_std))

"""## Eigen Descomposition

Eigenvectors and eigenvalues of a covariance (or correlation) matrix represent the “core” of a PCA:

> * The Eigenvectors (principal components) determine the directions of the new feature space, and the eigenvalues determine their magnitude.

### Covariance Matrix

Covariance is a matrix **dxd** where ***d is the number of features***, in this case will be a **4x4** matrix
"""

cov_mat= np.cov(X_std, rowvar=False)

print("\n\nCovariance matrix\n\n{}".format(cov_mat))

"""### Eigenvectors and Eigenvalues computation

To have a deeper understand is helpful some of the basic concepts of linear algebra
"""

cov_mat = np.cov(X_std.T) 
eig_vals, eig_vecs = np.linalg.eig(cov_mat)

"""Eigenvectors"""

print("\n\nEigenvectors\n\n{}".format(eig_vecs))

"""Eigenvalues"""

print("\n\nEigenvectors\n\n{}".format(eig_vals))

"""### Eigen Vectors verification

As we know that the sum of the square of each value in an Eigenvector is 1. We can make a quick check to see if eigenvectors are computed correctly
"""

print("\n\nEigenvectors verification\n\n{}".format(sum(eig_vecs**2)))

"""## Selecting The Principal Components

Make a list of (eigenvalue, eigenvector) tuples
"""

eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]
print("\n\nEigen pais\n\n{}".format(eig_pairs))

"""Sort the tuple from high to low"""

eig_pairs.sort() 
eig_pairs.reverse()
print("\n\nSorted eigen pairs\n\n{}".format(eig_pairs))

"""A useful measure to select the principal components is the so-called “explained variance,” which can be calculated from the eigenvalues. With this we will select the components with the higher weigth to approximate to **p-value**"""

tot = sum(eig_vals)
var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]
print("\n\nExplained variance\n\n{}".format(var_exp))

"""In this case we will select the first two components where they sum is ~ 95.80 (**p-value -> 0.95**)

## Construct the projection matrix W

Now let's reduce the dimension of dataset using top-k eigenvectors, so our dataset will be **dxk-dimensional** dimension
"""

matrix_w = np.hstack(
    (eig_pairs[0][1].reshape(4,1), eig_pairs[1][1].reshape(4,1))
)
print("\n\nProjection matrix W\n\n{}".format(matrix_w))

"""##Projection Onto the New Feature Space

To transform our samples to the new subspace we should use **Y = XxW**
"""

Y = X_std.dot(matrix_w) 
principalDf = pd.DataFrame(data = Y , columns = ['principal_component_1', 'principal_component_2']) 
print("\n\nNew feature space\n\n{}".format(principalDf.head()))

"""Let's combine the target **y** to dataframe"""

finalDf = pd.concat([principalDf,pd.DataFrame(y,columns = ['species'])], axis = 1) 
print("\n\nFinal features-target space\n\n{}".format(finalDf.head()))

"""##Visualize 2D Projection"""

fig = plt.figure(figsize = (8,5))
ax = fig.add_subplot(1,1,1)
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 Component PCA', fontsize = 20)
targets = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']
colors = ['r', 'g', 'b']

for target, color in zip(targets,colors): 
    indicesToKeep = finalDf['species'] == target
    ax.scatter(
        finalDf.loc[indicesToKeep, 'principal_component_1'],
        finalDf.loc[indicesToKeep, 'principal_component_2'],
        c = color,
        s = 50
    )
    ax.legend(targets)
    ax.grid()

plt.savefig("script/pca.png")

"""## Compute PCA with library

We can calculate **PCA** importing PCA from sklearn.decomposition.

> * PCA(n_components=N) # N -> Number of components we want to keep
> * PCA(pValue) # pValue -> Number of the variance we want to keep in **.f** (.95) notation, in this way PCA will return the number N of components used
"""

print("\n\nPCA using Sklearn")

pca = PCA(.95)

"""Projection Onto the New Feature Space"""

principalComponents = pca.fit_transform(X_std)

"""Gettind number of components wih numpy.ndarray.shape[1]"""

print("\n\nNumber of components\n\n{}".format(principalComponents.shape[1]))

"""Create a dataframe with the new components"""

principalDf = pd.DataFrame(
    data = principalComponents,
    columns = ['principal_component_1', 'principal_component_2']
)

print("\n\nPrincipal components\n\n{}".format(principalDf.head()))

"""Let's combine the target **y** to dataframe"""

finalDf = pd.concat([principalDf, finalDf[['species']]], axis = 1)
print("\n\nFinal dataframe\n\n{}".format(finalDf.head()))